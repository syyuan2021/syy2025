---
title: 'Deepseek: V3'
date: 2025-02-07
permalink: /2025/02/07/blog-post-3/
tags:
  - LLM
  - deepseek
  - low-precision training
  - FP8 mixed precision training
  - knowledge distillation
  - Multi-Token Prediction
---

This blog mainly record my learning process of deepseek series publications and technique reports. 

# 0. Main Objective 
achieve efficient training
## 0.1 FP8 mixed precision training
V3 propose a fine-grained mixed precision framework utilizing the FP8 data format for training DeepSeek-V3.
Previous low-precision training limitations: the presence of outliers in activations, weights, and gradients [1]
### 0.1.1 Mixed Precision Framework
### 0.1.2 Improved Precision from Quantization and Multiplication
#### 0.1.2.1 Fine-Grained Quantization
#### 0.1.2.2 Increasing Accumulation Precision.

## 0.2 Knowledge distillation

## 0.3 Multi-Token Prediction
Inspried by [2], V3 proposed MTP, and has the following advantages:
On the one hand, an MTP objective densifies the training signals and may improve data efficiency. 
On the other hand, MTP may enable the model to pre-plan its representations for better prediction of future tokens.

## 0.4 Auxiliary-loss-free load balancing


# 1. Model Architecture 
## 1.1 MLA
## 1.2 DeepSeekMOE
## 1.3 auxiliary-loss-free load balancing

# 2. Pre-training

# 3. Post-training
## 3.1 Knowledge Distillation from DeepSeek-R1
V3 introduce an innovative methodology to distill reasoning capabilities from the long- Chain-of-Thought (CoT) model

## 3.2 RL
V3 employ a rule-based Reward Model (RM) and a model-based RM in our RL process.
### 3.2.1 Rule-based Reward Model (RM)
### 3.2.2 model-based RM

### 3.2.3 GPRO




Reference:
[1] Liu, Aixin, et al. "Deepseek-v3 technical report." arXiv preprint arXiv:2412.19437 (2024).
[2] Gloeckle, Fabian, et al. "Better & faster large language models via multi-token prediction." arXiv preprint arXiv:2404.19737 (2024).


