---
title: 'Transformer:  Attention Variants'
date: 2025-01-31
permalink: /2025/01/31/blog-post-1/
tags:
  - tranformer
  - attention
---

This blog mainly record my learning process of transformer attention part. Including multi-head attention (MHA) in the 'Attention is all you need' paper, and its following variants, 
such as multi-head latent attention (MLA), multi-query attention (MQA), grouped query attention (GQA). In paper 'Attention is all you need', the input tokens X (e.g., from word embeddings) serve as the initial source for key, query, values (in shape of batch_size, seq_len, emb_dim). 

# Preliminary: Key-Value (KV) cache

## What is KV cache
**KV Cache (Key-Value Cache)** is a memory optimization technique used in transformer-based models, particularly for autoregressive decoding in Large Language Models (LLMs) like GPT-3, GPT-4, and LLaMA. It stores previously computed keys (K) and values (V) for self-attention layers, allowing the model to efficiently process long sequences without recomputing attention over past tokens.
**Heavy Key-Value (KV) cache** refers to a scenario in attention-based models, particularly transformer architectures, where the **key (K)** and **value (V)** tensors consume a large amount of memory during inference or training. This is particularly relevant for applications like Large Language Models (LLMs), where attention computations require **storing and accessing previous K and V states** efficiently.
In transformer-based models, self-attention requires computing attention over past tokens. Instead of recomputing **K, V** for all previous tokens at every step, models cache them to improve efficiency. In autoregressive inference (e.g., GPT models), at each step only new queries **Q** are computed, while previously computed **K, V** are retrieved from cache. This reduces redundant computations and speeds up inference.

## Why is KV Cache "Heavy"?

KV cache can become **"heavy"** due to several factors:

### **Memory Scaling with Sequence Length**
- Keys (**K**) and Values (**V**) grow with sequence length (**O(N) memory per layer**).
- If a model has **64 attention layers** and each KV cache entry is **4KB per token**, storing **10K tokens** results in **gigabytes of memory usage**.

### **Multi-Head Attention (MHA) Overhead**
- Since **each attention head** stores a **separate KV cache**, memory usage scales with the number of heads.
- **Example:** A **12-head model** requires **12Ã— more KV cache storage** than a single-head model.

### **Batch Size Effects**
- For **batched inference**, each instance maintains a **separate KV cache**, significantly increasing memory consumption.

### **High-Resolution KV Storage in Some Models**
- Some architectures, such as **Vision Transformers** and **multimodal transformers**, use **higher-dimensional KV embeddings**, further increasing KV cache size.




## How to Reduce Heavy KV Cache?

# 1. Multi-Head Attention (MHA)
## 1.1 Learnable Linear Transformations

Instead of using **X** (input embeddings) directly, separate **learnable linear transformations** are applied to obtain **Q, K, and V**:

$$
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
$$

where:
- \( X \) is the input token embedding of shape \((batch, seq\_len, d_{model}) \).
- \( W_Q, W_K, W_V \) are **learnable weight matrices** that project embeddings into **query, key, and value spaces**.
- The resulting **Q, K, and V** are of shape \((batch, seq\_len, d_{model}) \).

  
## 1.2 Splitting into Multiple Heads
Multi-head attention (MHA) splits **keys (K), queries (Q), and values (V)** into multiple heads (e.g., 8 heads). Instead of passing the same K, Q, and V to all heads, MHA **projects and splits** them into lower-dimensional subspaces. For example, if the **embedding dimension = 512** and we set **num_heads = 8**, each head receives: **subhead_dim = 64**
For example, if the **embedding dimension** is **512** and we set **num_heads = 8**, each head receives:

$$
\text{head_dim} = \frac{d_{\text{model}}}{\text{num_heads}} = \frac{512}{8} = 64
$$

To ensure the embedding can be evenly divided across multiple heads, **the embedding dimension must be divisible by the number of heads**, meaning:

$$
512 \mod 8 = 0
$$

Each head independently computes **scaled dot-product attention** before the results are concatenated and projected back into the original embedding space.





# 2.  MQA


# 3.  GQA

# 4. Multi-Head Latent Attention (MLA)
Multi-Head Latent Attention aims to boost inference efficiency. 


