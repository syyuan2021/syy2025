---
title: 'Transformer:  Attention Variants'
date: 2025-01-31
permalink: /2025/01/31/blog-post-1/
tags:
  - tranformer
  - attention
---

This blog mainly record my learning process of transformer attention part. Including multi-head attention (MHA) in the 'Attention is all you need' paper, and its following variants, 
such as multi-head latent attention (MLA), multi-query attention (MQA), grouped query attention (GQA). In paper 'Attention is all you need', tokens are duplicated as key, query, values, (in shape of batch_size, seq_len, emb_dim). 
1. MHA  # ![Multi-Head Attention](/images/mha_diagram.png)
Multi-head attention is to pass duplicated K, Q, V into multi-heads (e.g. 8 head).

2. MLA

3. MQA

4. GQA


