---
title: 'Transformer:  Attention Variants'
date: 2025-01-31
permalink: /2025/01/31/blog-post-1/
tags:
  - tranformer
  - attention
---

This blog mainly record my learning process of transformer attention part. Including multi-head attention (MHA) in the 'Attention is all you need' paper [1], and its following variants, 
such as multi-head latent attention (MLA), multi-query attention (MQA), grouped query attention (GQA). In paper 'Attention is all you need', the input tokens X (e.g., from word embeddings) serve as the initial source for key, query, values (in shape of batch_size, seq_len, emb_dim). 

# Preliminary: Key-Value (KV) cache

## Prelim 1: What is KV cache
**KV Cache (Key-Value Cache)** is a memory optimization technique used in transformer-based models, particularly for autoregressive decoding in Large Language Models (LLMs) like GPT-3, GPT-4, and LLaMA. It stores previously computed keys (K) and values (V) for self-attention layers, allowing the model to efficiently process long sequences without recomputing attention over past tokens.
**Heavy Key-Value (KV) cache** refers to a scenario in attention-based models, particularly transformer architectures, where the **key (K)** and **value (V)** tensors consume a large amount of memory during inference or training. This is particularly relevant for applications like Large Language Models (LLMs), where attention computations require **storing and accessing previous K and V states** efficiently.
In transformer-based models, self-attention requires computing attention over past tokens. Instead of recomputing **K, V** for all previous tokens at every step, models cache them to improve efficiency. In autoregressive inference (e.g., GPT models), at each step only new queries **Q** are computed, while previously computed **K, V** are retrieved from cache. This reduces redundant computations and speeds up inference.

## Prelim 2: Why is KV Cache "Heavy"?

KV cache can become **"heavy"** due to several factors:

### **Memory Scaling with Sequence Length**
- Keys (**K**) and Values (**V**) grow with sequence length (**O(N) memory per layer**).
- If a model has **64 attention layers** and each KV cache entry is **4KB per token**, storing **10K tokens** results in **gigabytes of memory usage**.

### **Multi-Head Attention (MHA) Overhead**
- Since **each attention head** stores a **separate KV cache**, memory usage scales with the number of heads.
- **Example:** A **12-head model** requires **12× more KV cache storage** than a single-head model.

### **Batch Size Effects**
- For **batched inference**, each instance maintains a **separate KV cache**, significantly increasing memory consumption.

### **High-Resolution KV Storage in Some Models**
- Some architectures, such as **Vision Transformers** and **multimodal transformers**, use **higher-dimensional KV embeddings**, further increasing KV cache size.



## Prelim 3: How to Reduce Heavy KV Cache?

Several optimizations exist to mitigate heavy KV cache:

### **Multi-Query Attention (MQA)**
- Instead of each attention head having its own **K** and **V**, all heads share a **single key-value representation**.
- **Reduces memory usage** while maintaining efficiency.

### **Grouped-Query Attention (GQA)**
- Groups attention heads to share **K** and **V**, instead of each head storing its own.
- **Reduces storage while maintaining flexibility** in attention computation.

### **Sliding Window Attention**
- Instead of caching **all past tokens**, this method **stores only a fixed number** of recent tokens.
- **Limits memory growth for long sequences** while preserving context.

### **Quantized KV Cache**
- Stores **K, V** in **lower precision** (e.g., **FP16** or **INT8**) instead of full **FP32** values.
- **Reduces memory footprint** while maintaining approximate accuracy.



# 1. Multi-Head Attention (MHA)
## 1.1 Learnable Linear Transformations

Instead of using **X** (input embeddings) directly, separate **learnable linear transformations** are applied to obtain **Q, K, and V**:

$$
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
$$

where:
- \( X \) is the input token embedding of shape \((batch, seq\_len, d_{model}) \).
- \( W_Q, W_K, W_V \) are **learnable weight matrices** that project embeddings into **query, key, and value spaces**.
- The resulting **Q, K, and V** are of shape \((batch, seq\_len, d_{model}) \).

  
## 1.2 Splitting into Multiple Heads
Multi-head attention (MHA) splits **keys (K), queries (Q), and values (V)** into multiple heads (e.g., 8 heads). Instead of passing the same K, Q, and V to all heads, MHA **projects and splits** them into lower-dimensional subspaces. For example, if the **embedding dimension = 512** and we set **num_heads = 8**, each head receives: **subhead_dim = 64**
For example, if the **embedding dimension** is **512** and we set **num_heads = 8**, each head receives:

$$
\text{head_dim} = \frac{d_{\text{model}}}{\text{num_heads}} = \frac{512}{8} = 64
$$

To ensure the embedding can be evenly divided across multiple heads, **the embedding dimension must be divisible by the number of heads**, meaning:

$$
512 \mod 8 = 0
$$

Each head independently computes **scaled dot-product attention** before the results are concatenated and projected back into the original embedding space.


# 2.   Multi-Query Attention (MQA)
Multi-head attention consists of multiple attention layers (heads) in parallel with different linear transformations on the queries, keys, values and outputs. Multi-query attention is identical except that the different heads share a single set of keys and values. [2]


# 3.  Grouped-query Attention (GQA) 
Grouped-query attention divides query heads into G groups, each of which shares a single key head and value head. GQA-G refers to grouped-query with G groups. GQA-1, with a single group and therefore single key and value head, is equivalent to MQA, while GQA-H, with groups equal to number of heads, is equivalent to MHA. [3]

# 4. Multi-Head Latent Attention (MLA)
Multi-Head Latent Attention aims to boost inference efficiency. Equipped with low-rank key-value joint compression, MLA achieves better performance than MHA, but requires a significantly smaller amount of KV cache. [4]


## References
[1] Vaswani, A., Shazeer, N., Parmar, N., et al. "Attention is All You Need." NeurIPS, 2017. Available at: https://arxiv.org/abs/1706.03762
[2] N. Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150,2019. URL http://arxiv.org/abs/1911.02150.
[3] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.
[4] Liu, Aixin, et al. "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model." arXiv preprint arXiv:2405.04434 (2024).

