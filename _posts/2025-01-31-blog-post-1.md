---
title: 'Transformer:  Attention Variants'
date: 2025-01-31
permalink: /2025/01/31/blog-post-1/
tags:
  - tranformer
  - attention
---

This blog mainly record my learning process of transformer attention part. Including multi-head attention (MHA) in the 'Attention is all you need' paper, and its following variants, 
such as multi-head latent attention (MLA), multi-query attention (MQA), grouped query attention (GQA). In paper 'Attention is all you need', the input tokens X (e.g., from word embeddings) serve as the initial source for key, query, values (in shape of batch_size, seq_len, emb_dim). 

# 1. Multi-Head Attention (MHA)
## Learnable Linear Transformations

Instead of using **X** (input embeddings) directly, separate **learnable linear transformations** are applied to obtain **Q, K, and V**:

$$
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
$$

where:
- \( X \) is the input token embedding of shape \((batch, seq\_len, d_{model}) \).
- \( W_Q, W_K, W_V \) are **learnable weight matrices** that project embeddings into **query, key, and value spaces**.
- The resulting **Q, K, and V** are of shape \((batch, seq\_len, d_{model}) \).

  
## Splitting into Multiple Heads
Multi-head attention (MHA) splits **keys (K), queries (Q), and values (V)** into multiple heads (e.g., 8 heads). Instead of passing the same K, Q, and V to all heads, MHA **projects and splits** them into lower-dimensional subspaces. For example, if the **embedding dimension = 512** and we set **num_heads = 8**, each head receives: **subhead_dim = 64**
For example, if the **embedding dimension** is **512** and we set **num_heads = 8**, each head receives:

$$
\text{head_dim} = \frac{d_{\text{model}}}{\text{num_heads}} = \frac{512}{8} = 64
$$

To ensure the embedding can be evenly divided across multiple heads, **the embedding dimension must be divisible by the number of heads**, meaning:

$$
512 \mod 8 = 0
$$

Each head independently computes **scaled dot-product attention** before the results are concatenated and projected back into the original embedding space.


2. MLA

3. MQA

4. GQA


