---
title: 'Transformer:  Attention Variants'
date: 2025-01-31
permalink: /2025/01/31/blog-post-1/
tags:
  - tranformer
  - attention
---

This blog mainly record my learning process of transformer attention part. Including multi-head attention (MHA) in the 'Attention is all you need' paper, and its following variants, 
such as multi-head latent attention (MLA), multi-query attention (MQA), grouped query attention (GQA). In paper 'Attention is all you need', the input tokens X (e.g., from word embeddings) serve as the initial source for key, query, values (in shape of batch_size, seq_len, emb_dim). But instead of using X directly, **separate learnable linear transformations** are applied to obtain **Q, K, and V**:
\[
Q = XW_Q, \quad K = XW_K, \quad V = XW_V
\]
where:
- \(X\) is the input token embedding of shape \((\text{batch}, \text{seq\_len}, d_{\text{model}})\).
- \(W_Q, W_K, W_V\) are **learnable weight matrices** that project embeddings into **query, key, and value spaces**.
- The resulting **Q, K, and V** are of shape \((\text{batch}, \text{seq\_len}, d_{\text{model}})\).

# 1. Multi-Head Attention (MHA)

Multi-head attention (MHA) splits **keys (K), queries (Q), and values (V)** into multiple heads (e.g., 8 heads). Instead of passing the same K, Q, and V to all heads, MHA **projects and splits** them into lower-dimensional subspaces. For example, if the **embedding dimension = 512** and we set **num_heads = 8**, each head receives: **subhead_dim = 64**



2. MLA

3. MQA

4. GQA


