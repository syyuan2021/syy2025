---
title: 'Transformer:  Attention Variants'
date: 2025-01-31
permalink: /2025/01/31/blog-post-1/
tags:
  - tranformer
  - attention
---

This blog mainly record my learning process of transformer attention part. Including multi-head attention (MHA) in the 'Attention is all you need' paper, and its following variants, 
such as multi-head latent attention (MLA), multi-query attention (MQA), grouped query attention (GQA). In paper 'Attention is all you need', tokens are duplicated as key, query, values, (in shape of batch_size, seq_len, emb_dim). 
# 1. Multi-Head Attention (MHA)

Multi-head attention (MHA) splits **keys (K), queries (Q), and values (V)** into multiple heads (e.g., 8 heads). Instead of passing the same K, Q, and V to all heads, MHA **projects and splits** them into lower-dimensional subspaces.

For example, if the **embedding dimension** is 512 and we set **num_heads = 8**, each head receives:



2. MLA

3. MQA

4. GQA


