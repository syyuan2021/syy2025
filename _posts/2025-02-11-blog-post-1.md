---
title: 'Distillation'
date: 2025-02-11
permalink: /2025/02/11/blog-post-1/
tags:
  - distillation
---

This blog mainly record my learning process of distillation.

# 1. Cross Entropy,  Entropy,  KL Divergence 
Suppose you need to describe a thing:
Cross Entropy: How many sentences are needed to describe the thing clearly.  $$H(P, Q) = - \sum{P(x) log Q(x)}$$

Entropy: Suppose you are 100% familiar with the thing, at least how mant sencentes you need to describe the thing clearly. $$H(P) = -\sum{P(x) log P(x)}$$

KL: The extra sentences in your description of the thing due to you are not familiar with the thing. $$D_{KL} (P|Q) = \sum{P(x) log \frac{P(x)}{Q(x)}}$$
