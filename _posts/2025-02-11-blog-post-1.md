---
title: 'Distillation'
date: 2025-02-11
permalink: /2025/02/11/blog-post-1/
tags:
  - distillation
---

This blog mainly record my learning process of distillation.

# 1. Cross Entropy,  Entropy,  KL Divergence 
Suppose you need to describe a thing:
Cross Entropy: How many sentences are needed to describe the thing clearly.  $$H(P, Q) = - \sum{P(x) log Q(x)}$$

Entropy: Suppose you are 100% familiar with the thing, at least how mant sencentes you need to describe the thing clearly. $$H(P) = -\sum{P(x) log P(x)}$$

KL: The extra sentences in your description of the thing due to you are not familiar with the thing.    $$D_{KL} (P|Q) = \sum{P(x) log \frac{P(x)}{Q(x)}}$$

Cross Entropy = Entropy + KL    
$$H(P, Q) = - \sum{P(x) log Q(x)}= -\sum{P(x) log P(x)} + \sum{P(x) log \frac{P(x)}{Q(x)}}$$

## 1.1 Small Case Study
Flip coin true distribution : [0.5, 0.5] for head and tail probability.
Predicted coin distribution: [0.8, 0.2] for head and tail probability.

Entropy:  $$H(P) = -(0.5 * log(0.5) + 0.5 * log(0.5)) = 0.693$$

