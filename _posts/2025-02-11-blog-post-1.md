---
title: 'Distillation'
date: 2025-02-11
permalink: /2025/02/11/blog-post-1/
tags:
  - distillation
---

This blog mainly record my learning process of distillation.

# 1. Cross Entropy,  Entropy,  KL Divergence 
Suppose you need to describe a thing:
Cross Entropy: How many sentences are needed to describe the thing clearly.  $$H(P, Q) = - \sum{P(x) log Q(x)}$$

Entropy: Suppose you are 100% familiar with the thing, at least how mant sencentes you need to describe the thing clearly. $$H(P) = -\sum{P(x) log P(x)}$$

KL: The extra sentences in your description of the thing due to you are not familiar with the thing.    $$D_{KL} (P|Q) = \sum{P(x) log \frac{P(x)}{Q(x)}}$$

Cross Entropy = Entropy + KL    
$$H(P, Q) = - \sum{P(x) log Q(x)}= -\sum{P(x) log P(x)} + \sum{P(x) log \frac{P(x)}{Q(x)}}$$

## 1.1 Small Case Study
Flip coin true distribution : $$[p_{head} = 0.5, p_{tail} = 0.5]$$ for head and tail probability.
Predicted coin distribution: $$[p_{head} = 0.7, p_{tail} = 0.3]$$ for head and tail probability.

Entropy:  $$H(P) = -[P_{head}(x) log P_{head}(x) + P_{tail}(x) log P_{tail}(x)] = -(0.5 * log(0.5) + 0.5 * log(0.5)) = 0.693$$

KL:        $$D_{KL}(P|Q) = [P_{head}(x) log \frac{P_{head}(x)}{P_{tail}(x)} + P_{tail}(x) log \frac{P_{tail}(x)}{P_{head}(x)}] = 0.7 * log\frac{0.7}{0.3} + 0.3 * log\frac{0.3}{0.7} = 0.339$$

Cross Entropy: $$H(P, Q) = - [P_{head} log P_{tail} + P_{tail} logP_{head}] = -[0.7 * log(0.3) + 0.3 * log(0.7)] = 0.9498$$

