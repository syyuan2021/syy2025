---
title: 'Distillation'
date: 2025-02-11
permalink: /2025/02/11/blog-post-1/
tags:
  - distillation
---

This blog mainly record my learning process of distillation.

# 1. Cross Entropy,  Entropy,  KL Divergence 
Suppose you need to describe a thing:
Cross Entropy: How many sentences are needed to describe the thing clearly.  $$H(P, Q) = - \sum{P(x) log Q(x)}$$

Entropy: Suppose you are 100% familiar with the thing, at least how mant sencentes you need to describe the thing clearly. $$H(P) = -\sum{P(x) log P(x)}$$

KL: The extra sentences in your description of the thing due to you are not familiar with the thing.    $$D_{KL} (P|Q) = \sum{P(x) log \frac{P(x)}{Q(x)}}$$

Cross Entropy = Entropy + KL    
$$H(P, Q) = - \sum{P(x) log Q(x)}= -\sum{P(x) log P(x)} + \sum{P(x) log \frac{P(x)}{Q(x)}}$$

## 1.1 Small Case Study
Flip coin true distribution : $$[P_{head} = 0.5, P_{tail} = 0.5]$$ for head and tail probability.
Predicted coin distribution: $$[Q_{head} = 0.7, Q_{tail} = 0.3]$$ for head and tail probability.

Entropy:  $$H(P) = -[P_{head}(x) log P_{head}(x) + P_{tail}(x) log P_{tail}(x)] = -[0.5 * log(0.5) + 0.5 * log(0.5)] = 0.693$$

KL:        $$D_{KL}(P|Q) = [P_{head}(x) log \frac{P_{head}(x)}{Q_{head}(x)} + P_{tail}(x) log \frac{P_{tail}(x)}{Q_{tail}(x)}] = 0.5 * log\frac{0.5}{0.7} + 0.5 * log\frac{0.5}{0.3} = 0.087$$

Cross Entropy: $$H(P, Q) = - [P_{head} log Q_{head} + P_{tail} log Q_{tail}] = -[0.5 * log(0.7) + 0.5 * log(0.3)] = 0.780$$

Cross Entropy = Entropy + KL = 0.693 + 0.087

# 2. Hard Label and Soft Label
hard label: [1, 0, 0]

soft label: [0.7, 0.2, 0.1]   : soft label is from softmax $$softmax(x) = \frac{exp^{x_{i}}}{\sum{exp^{x_{j}}_{j}^{J}}} $$

