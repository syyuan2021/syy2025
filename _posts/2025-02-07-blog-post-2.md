---
title: 'Deepseek: V2'
date: 2025-02-07
permalink: /2025/02/07/blog-post-2/
tags:
  - LLM
  - deepseek
  - MLA
  - DeepseekMOE
  - GRPO
---

This blog mainly record my learning process of deepseek series second paper [1] focus on Multi-head Latent Attention (MLA) and DeepseekMOE. 
# Model Architecture 
## DS-V2 adopted new MOE : DeepseekMOE with following differences with LLAMA3 dense transformer [2]. 


# RL
## Group Relative Policy Optimization (GRPO) [3]















Reference:
[1] Liu, Aixin, et al. "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model." arXiv preprint arXiv:2405.04434 (2024).
[2] Dubey, Abhimanyu, et al. "The llama 3 herd of models." arXiv preprint arXiv:2407.21783 (2024).
[3] Shao, Zhihong, et al. "Deepseekmath: Pushing the limits of mathematical reasoning in open language models." arXiv preprint arXiv:2402.03300 (2024).
