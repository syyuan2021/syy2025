---
title: 'Deepseek: V2'
date: 2025-02-07
permalink: /2025/02/07/blog-post-2/
tags:
  - LLM
  - deepseek
  - MLA
  - economical training
  - efficient inference
  - long context
---

This blog mainly record my learning process of deepseek series second paper [1] focus on Multi-head Latent Attention (MLA) and DeepseekMOE. 
context length of 128K tokens

# Main Objective 
economical training and efficient inference 

## MLA


# Model Architecture 
## 1. MLA
low-rank key-value joint compression to eliminate the bottleneck of inference-time key-value cache, thus supporting efficient inference.


## 2. DeepSeekMoE [2]


# 2. Pre-training

# 3. Post-raining: SFT and DPO
## 3.1 SFT 

## 3.2 RL
## Group Relative Policy Optimization (GRPO) [3]




Reference:
[1] Liu, Aixin, et al. "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model." arXiv preprint arXiv:2405.04434 (2024).
[2] Dai, Damai, et al. "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models." arXiv preprint arXiv:2401.06066 (2024).
[3] Shao, Zhihong, et al. "Deepseekmath: Pushing the limits of mathematical reasoning in open language models." arXiv preprint arXiv:2402.03300 (2024).















Reference:
[1] Liu, Aixin, et al. "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model." arXiv preprint arXiv:2405.04434 (2024).
[2] Dubey, Abhimanyu, et al. "The llama 3 herd of models." arXiv preprint arXiv:2407.21783 (2024).
[3] Shao, Zhihong, et al. "Deepseekmath: Pushing the limits of mathematical reasoning in open language models." arXiv preprint arXiv:2402.03300 (2024).
