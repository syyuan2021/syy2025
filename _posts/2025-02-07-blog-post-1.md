---
title: 'Deepseek: DS-LLM'
date: 2025-02-07
permalink: /2025/02/07/blog-post-1/
tags:
  - LLM
  - deepseek
  - different SFT strategies
  - DPO
---

This blog mainly record my learning process of deepseek series first paper [1] which is about different SFT strategies and direct preference optimization (DPO)[2].
# Model Architecture 
## DS-LLM inherit LLaMA architecture with learning rate scheduler improvement. 

# RL
## direct preference optimization (DPO) [2]













Reference:
[1] Bi, Xiao, et al. "Deepseek llm: Scaling open-source language models with longtermism." arXiv preprint arXiv:2401.02954 (2024).
[2] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." Advances in Neural Information Processing Systems 36 (2024).
