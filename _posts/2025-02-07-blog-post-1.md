---
title: 'Deepseek: DS-LLM'
date: 2025-02-07
permalink: /2025/02/07/blog-post-1/
tags:
  - LLM
  - deepseek
  - different SFT strategies
  - richness datasets
  - diversified datasets
---

This blog mainly record my learning process of deepseek series first paper [1] which is about different SFT strategies and direct preference optimization (DPO)[2].

# Main Objective 
Comprehensively enhance the richness and diversity of the dataset

## Data Processing
Three essential stages: deduplication, filtering, and remixing. 


# 1. Model Architecture 
DS-LLM inherit LLaMA architecture with learning rate scheduler improvement. 
## 1.1 Pre-Norm structure with RMSNorm
## 1.2 SwiGLU
## 1.3 Rotary Embedding for positional encoding (ROPE)
## 1.4 Grouped- Query Attention (GQA) 

# 2. Pre-training



# 3. Post-raining: SFT and DPO
## 3.1 SFT 


## 3.2 RL
direct preference optimization (DPO) [2]













Reference:
[1] Bi, Xiao, et al. "Deepseek llm: Scaling open-source language models with longtermism." arXiv preprint arXiv:2401.02954 (2024).
[2] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." Advances in Neural Information Processing Systems 36 (2024).
