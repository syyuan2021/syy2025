---
title: 'Transformer: Normalization'
date: 2025-02-03
permalink: /2025/02/03/blog-post-8/
tags:
  - tranformer
  - Layer Normalization
  - Batch Normalization
  - RMSNrom
---

This blog mainly record my learning process of transformer normalizarion part. Including LayerNorm, BatchNorm, RMSNorm. 

# Layer Normalization (LayerNorm)
## LayerNorm works on sample or data level.

# Root Mean Square Normalization (RMSNorm)
## RMSNorm works on sample or data level, but more efficient than LayerNorm since it rescale on the RMS of expectation, which do not need to re-centering. 

# Batch Normalization (BatchNorm)
## Works on feature level.

