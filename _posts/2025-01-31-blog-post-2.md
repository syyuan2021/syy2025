---
title: 'Transformer:  Positional Encoding Variants'
date: 2025-01-31
permalink: /2025/01/31/blog-post-2/
tags:
  - tranformer
  - positional encoding
---

This blog mainly record my learning process of transformer positional encoding part. Including sine-cosine PE in the 'Attention is all you need' paper, and its following variants, 
such as rotary positional embedding (ROPE). [1] Unlike RNN, attention-based model treats all tokens simultaneously. Without explicit positional encoding, the model would treat "The cat sat" the same as "Sat cat the," which is undesirable. To incorporate position information, different approaches define $$f_q$$, $$f_k$$, $$f_v$$ in different ways. 
# 1. What is positional embedding?
In Transformers, positional encoding is essential since the model lacks recurrence or convolution to understand the order of tokens. The traditional approach (Vaswani et al., 2017) adds absolute positional encoding (sinusoidal functions) to token embeddings.
## 1.1 Basic structure of sequence
### 1.1.1 Sequence Definition

Let $$S_N = \{ w_i \}_{i=1}^{N}$$ be a sequence of **N** input tokens, where $$w_i$$ is the $$i^{th}$$ token.  
Each token $$w_i$$ is mapped to a word embedding $$x_i$$ from an embedding matrix. This results in a sequence of embeddings:

$$
E_N = \{ x_i \}_{i=1}^{N}, \quad x_i \in \mathbb{R}^d
$$

where $$x_i$$ is a **d** -dimensional vector representation of the token $$w_i$$ and initially has no positional information.

### 1.1.2 Transforming Word Embeddings into Query, Key, and Value Representations

To perform self-attention, the Transformer applies separate transformation functions to generate query **Q**, **K**, and **V** representations:

$$
q_m = f_q(x_m, m), \quad k_n = f_k(x_n, n), \quad v_n = f_v(x_n, n)
$$

where:

- $$f_q$$, $$f_k$$, and $$f_v$$ are functions that incorporate **position information** into the embeddings.
- $$q_m$$ is the **query vector** for the token at position **m**.
- $$k_n$$ is the **key vector** for the token at position **n**.
- $$v_n$$ is the **value vector** for the token at position **n**.

### 1.1.3 Role of position embedding in attention computation
Attention computes the attention scores using the scaled dot product as follows:

$$
a_{m, n} = \frac{\text{exp}(q_{m}k_{n}^{T}/ \sqrt{d})}{\sum_{j=1}^{N}{\text{exp}(q_{m}k_{j}^{T}/ \sqrt{d}})}
$$

where:
- $$q_{m}k_{n}^{T}$$ meansures the similarity between token $$m$$ and $$n$$.
- The denominator ensures a probability distribution over all tokens.
- $$v_n$$ is then weighted by attention scores to produce the final representation: $$output_n = \sum_{n=1}^{N}{a_{m,n} v_n}$$

Positional encodings directly affect the computation of $$q_{m}k_{n}^{T}$$ , influencing which tokens attend to each other. RoPE preserves relative positional information, unlike absolute encoding, which only represents token order.


# 2. Absolute position embedding
Adds fixed positional encodings to token embeddings (e.g., sinusoidal functions in [2]).

$$
f_q (x_m, m) = W_Q(x_m + p_m) , \quad   f_k(x_n, n) = W_K(x_n + p_n)
$$

where:
- $$p_m$$, $$p_n$$ are absolute position encodings.


# 3. Relative position embedding
Uses trainable or predefined functions to encode relative position differences.

$$
f_k(x_n, n) = W_K(x_n) + p_{(m-n)}
$$

where:

- $$p_{(m-n)}$$ is depends on the relative position $$m-n$$ rather than absolute position.


# 4. Rotary position embedding
Instead of adding positional encodings, RoPE multiplies embeddings with a rotation matrix. It modify $$f_q$$, and $$f_k$$ to inject position via rotation. This allows the self-attention mechanism to naturally encode relative positions into the dot product.

$$
f_q (x_m, m) = W_Q(x_m) \times e^{i \times m_\theta} , \quad   f_k(x_n, n) = W_K(x_n) \times e^{i \times n_\theta}
$$

where:

- $$p_{(m-n)}$$ is depends on the relative position $$m-n$$ rather than absolute position.





## References:
[1] Su, Jianlin, et al. "Roformer: Enhanced transformer with rotary position embedding." Neurocomputing 568 (2024): 127063.

[2] Vaswani, A., Shazeer, N., Parmar, N., et al. "Attention is All You Need." NeurIPS, 2017. Available at: https://arxiv.org/abs/1706.03762
