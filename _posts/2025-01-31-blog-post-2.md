---
title: 'Transformer:  Positional Encoding Variants'
date: 2025-01-31
permalink: /2025/01/31/blog-post-2/
tags:
  - tranformer
  - positional encoding
---

This blog mainly record my learning process of transformer positional encoding part. Including sine-cosine PE in the 'Attention is all you need' paper, and its following variants, 
such as rotary positional embedding (ROPE). [1]
# 1. What is positional embedding?
In Transformers, positional encoding is essential since the model lacks recurrence or convolution to understand the order of tokens. The traditional approach (Vaswani et al., 2017) adds absolute positional encoding (sinusoidal functions) to token embeddings.
## 1.1 Basic structure of sequence
### 1.1.1 Sequence Definition
Let \(S_N = \{ w_i \}_{i=1}^{N}\) be a sequence of \(N\) input tokens, where \(w_i\) is the \(i^{th}\) token.  
Each token $w_i$ is mapped to a word embedding \(x_i\) from an embedding matrix. This results in a sequence of embeddings:

$$
E_N = \{ x_i \}_{i=1}^{N}, \quad x_i \in \mathbb{R}^d
$$

where $x_i$ is a $d$-dimensional vector representation of the token $w_i$ and initially has no positional information.


# 2. Absolute position embedding


# 3. Relative position embedding

# 4. Rotary position embedding





## References:
[1] Su, Jianlin, et al. "Roformer: Enhanced transformer with rotary position embedding." Neurocomputing 568 (2024): 127063.
