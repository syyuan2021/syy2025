---
title: 'Transformer:  Positional Encoding Variants'
date: 2025-01-31
permalink: /2025/01/31/blog-post-2/
tags:
  - tranformer
  - positional encoding
---

This blog mainly record my learning process of transformer positional encoding part. Including sine-cosine PE in the 'Attention is all you need' paper, and its following variants, 
such as rotary positional embedding (ROPE). [1]
# 1. What is positional embedding?
In Transformers, positional encoding is essential since the model lacks recurrence or convolution to understand the order of tokens. The traditional approach (Vaswani et al., 2017) adds absolute positional encoding (sinusoidal functions) to token embeddings.
## 1.1 Basic structure of sequence
### 1.1.1 Sequence Definition

Let $$ S_N = \{ w_i \}_{i=1}^{N} $$ be a sequence of \( N \) input tokens, where \( w_i \) is the \( i^{th} \) token.  
Each token \( w_i \) is mapped to a word embedding \( x_i \) from an embedding matrix. This results in a sequence of embeddings:

$$
E_N = \{ x_i \}_{i=1}^{N}, \quad x_i \in \mathbb{R}^d
$$

where \( x_i \) is a \( d \)-dimensional vector representation of the token \( w_i \) and initially has no positional information.

### 1.1.2 Transforming Word Embeddings into Query, Key, and Value Representations

To perform self-attention, the Transformer applies separate transformation functions to generate query (\( Q \)), key (\( K \)), and value (\( V \)) representations:

$$
q_m = f_q(x_m, m), \quad k_n = f_k(x_n, n), \quad v_n = f_v(x_n, n)
$$

where:

- \( f_q \), \( f_k \), and \( f_v \) are functions that incorporate **position information** into the embeddings.
- \( q_m \) is the **query vector** for the token at position \( m \).
- \( k_n \) is the **key vector** for the token at position \( n \).
- \( v_n \) is the **value vector** for the token at position \( n \).



# 2. Absolute position embedding


# 3. Relative position embedding

# 4. Rotary position embedding





## References:
[1] Su, Jianlin, et al. "Roformer: Enhanced transformer with rotary position embedding." Neurocomputing 568 (2024): 127063.
